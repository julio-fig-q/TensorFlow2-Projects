{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1UsYB--LP3BnY0mVMJy3boNsIIYtnBpnn","authorship_tag":"ABX9TyND0Qpkg2GHcNdydpOVVmYr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# The compile method is a method (function) of the model object. And it basically allow us to set functionalities of our built model, in order to train it.\n","\n","At this point this should be on the GitHub Repository btw.\n"],"metadata":{"id":"K3aprHFkLky5"}},{"cell_type":"markdown","source":["## **Binary classificaiton network**"],"metadata":{"id":"vZy3oN7ZL2eq"}},{"cell_type":"code","source":["# As usual we import what we will need\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense"],"metadata":{"id":"VPxnpBWjMjdR","executionInfo":{"status":"ok","timestamp":1664640484687,"user_tz":-60,"elapsed":3374,"user":{"displayName":"Julio Figueroa","userId":"08124126545682677430"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["model = Sequential([\n","    Dense(64, activation='elu', input_shape=(32,)),   # Takes a 1d tensor of size 32, exponential linear activation and 64 units\n","    Dense(1, activation='sigmoid')                    # 1 neuron with sigmoid activation\n","])"],"metadata":{"id":"iMl_LSnYMscZ","executionInfo":{"status":"ok","timestamp":1664640485097,"user_tz":-60,"elapsed":413,"user":{"displayName":"Julio Figueroa","userId":"08124126545682677430"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## We now define the optimizer and loss-function for the network"],"metadata":{"id":"QUfHDr_-NI1S"}},{"cell_type":"code","source":["# The compiler\n","\n","model.compile(\n","    optimizer='sgd',            # Stochastic gradient descent\n","    loss='binary_crossentropy', # Ideal for this task / network\n","    metrics=['accuracy']        # Set of performance metrics to keep track of while it is training\n","                                # calculated at each epoch;\n","                                # number of passes of the entire training dataset the machine learning algorithm has completed\n",")"],"metadata":{"id":"9Lu9K_BrNPmr","executionInfo":{"status":"ok","timestamp":1664640485097,"user_tz":-60,"elapsed":2,"user":{"displayName":"Julio Figueroa","userId":"08124126545682677430"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## There is loads of flexibility so tere is lots of options for these spcifications."],"metadata":{"id":"7M_IsPgmUAgB"}},{"cell_type":"code","source":["model.compile(\n","    optimizer='sgd',            # 'adam', 'rmsprop', 'adadelta', etc\n","    loss='binary_crossentropy', # 'mean_squared_error', 'categorical_crossentropy'\n","    metrics=['accuracy', 'mae'] # mean absolute error\n",")"],"metadata":{"id":"UQzuQMgiTd7J","executionInfo":{"status":"ok","timestamp":1664640803679,"user_tz":-60,"elapsed":354,"user":{"displayName":"Julio Figueroa","userId":"08124126545682677430"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Each of those strings above are technically objects or functions, which can also be used directly:"],"metadata":{"id":"Z_g3A1TZUEfL"}},{"cell_type":"code","source":["model.compile(\n","    optimizer=tf.keras.optimizers.SGD(),  # SGD Object from Keras\n","    loss=tf.keras.losses.BinaryCrossentropy(), # Loss object from keras\n","    metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.MeanAbsoluteError()]\n",")"],"metadata":{"id":"f_oa6xtiUMFb","executionInfo":{"status":"ok","timestamp":1664640804589,"user_tz":-60,"elapsed":1,"user":{"displayName":"Julio Figueroa","userId":"08124126545682677430"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## The reason of why would one want to use the objects instead of the string that calls them  is so that we can have more control over them and use things outside the default: "],"metadata":{"id":"lr08CizBUs-E"}},{"cell_type":"code","source":["model = Sequential([\n","    Dense(64, activation='elu', input_shape=(32,)),   # Takes a 1d tensor of size 32, exponential linear activation and 64 units\n","    Dense(1, activation='linear')                    # 1 neuron with sigmoid activation\n","])\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001,\n","                                      momentum=0.9, nesterov=True),\n","    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","    metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.7),\n","             tf.keras.metrics.MeanAbsoluteError()]\n",")"],"metadata":{"id":"2Ya-qFkdU3l1","executionInfo":{"status":"ok","timestamp":1664641372059,"user_tz":-60,"elapsed":199,"user":{"displayName":"Julio Figueroa","userId":"08124126545682677430"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Notice how above the activation function of the last layer is *linear*, i.e., no longer an activation function.\n","\n","This means that we have assigned the task of \"squeezing\" the output of the network through the sigmoid activation function to be processed by the _loss_ (see from_logits=True).\n","\n","No mathematical difference, but more stable."],"metadata":{"id":"KuAN4h_aW4zu"}},{"cell_type":"markdown","source":["## Now we've seen how to build NN as sequential models and set the optimizer, loss, and metrics using the **compile** method. We got all needed to start training."],"metadata":{"id":"wVIMR2HlXemS"}}]}